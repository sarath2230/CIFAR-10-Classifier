{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "vxWpggp9dRKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvZUx8uErkt0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4zkqPIJr3UI",
        "outputId": "dd9021b6-3a79-4e87-fee9-1e3f45db09a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc2dqZRCMQqZ",
        "outputId": "d5caefc2-81b0-4830-da20-dcfa52e7d1dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuS25lZ3MTOv",
        "outputId": "5a03adb7-20ed-4a20-c16c-81db75e8a01a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H36kzoUlr-WD"
      },
      "outputs": [],
      "source": [
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0qYaBnZsBmy"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoded format\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFRc6BvrsIJK"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viUcAqjvsLdx",
        "outputId": "8d7b6299-70a7-404a-c569-4c17ed86f229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the ResNet50 model without the top layers and freeze its layers\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sua_Mw7NsRwN"
      },
      "outputs": [],
      "source": [
        "# Add custom layers\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "# Compile the model with Adam optimizer\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "D77zB9PY0UWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q9sZaq0sqdy"
      },
      "outputs": [],
      "source": [
        "# Callbacks for learning rate scheduling and early stopping\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3YZb8Disv_v",
        "outputId": "47f8429b-208a-4d73-983f-02bdeb3b8394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 66ms/step - accuracy: 0.1739 - loss: 2.3017 - val_accuracy: 0.3565 - val_loss: 1.8687 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 54ms/step - accuracy: 0.2470 - loss: 2.0778 - val_accuracy: 0.3714 - val_loss: 1.8395 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.2651 - loss: 2.0364 - val_accuracy: 0.3761 - val_loss: 1.8169 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.2759 - loss: 2.0041 - val_accuracy: 0.3796 - val_loss: 1.8029 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 52ms/step - accuracy: 0.2784 - loss: 1.9945 - val_accuracy: 0.3870 - val_loss: 1.7884 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.2859 - loss: 1.9795 - val_accuracy: 0.3860 - val_loss: 1.7783 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 51ms/step - accuracy: 0.2879 - loss: 1.9761 - val_accuracy: 0.3972 - val_loss: 1.7737 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - accuracy: 0.2885 - loss: 1.9712 - val_accuracy: 0.3989 - val_loss: 1.7555 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.2891 - loss: 1.9628 - val_accuracy: 0.4024 - val_loss: 1.7571 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 57ms/step - accuracy: 0.2959 - loss: 1.9570 - val_accuracy: 0.3999 - val_loss: 1.7521 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 52ms/step - accuracy: 0.2986 - loss: 1.9506 - val_accuracy: 0.4050 - val_loss: 1.7493 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.2966 - loss: 1.9477 - val_accuracy: 0.4080 - val_loss: 1.7413 - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 51ms/step - accuracy: 0.3012 - loss: 1.9433 - val_accuracy: 0.4058 - val_loss: 1.7407 - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - accuracy: 0.3019 - loss: 1.9462 - val_accuracy: 0.4102 - val_loss: 1.7361 - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 51ms/step - accuracy: 0.3002 - loss: 1.9414 - val_accuracy: 0.4073 - val_loss: 1.7330 - learning_rate: 1.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 53ms/step - accuracy: 0.3036 - loss: 1.9445 - val_accuracy: 0.4123 - val_loss: 1.7274 - learning_rate: 1.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.3067 - loss: 1.9294 - val_accuracy: 0.4124 - val_loss: 1.7291 - learning_rate: 1.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.3029 - loss: 1.9398 - val_accuracy: 0.4133 - val_loss: 1.7228 - learning_rate: 1.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.3042 - loss: 1.9345 - val_accuracy: 0.4147 - val_loss: 1.7224 - learning_rate: 1.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.3032 - loss: 1.9310 - val_accuracy: 0.4187 - val_loss: 1.7163 - learning_rate: 1.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 54ms/step - accuracy: 0.3133 - loss: 1.9260 - val_accuracy: 0.4117 - val_loss: 1.7238 - learning_rate: 1.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 59ms/step - accuracy: 0.3123 - loss: 1.9230 - val_accuracy: 0.4168 - val_loss: 1.7135 - learning_rate: 1.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 52ms/step - accuracy: 0.3088 - loss: 1.9263 - val_accuracy: 0.4167 - val_loss: 1.7081 - learning_rate: 1.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 52ms/step - accuracy: 0.3056 - loss: 1.9344 - val_accuracy: 0.4173 - val_loss: 1.7042 - learning_rate: 1.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3139 - loss: 1.9221 - val_accuracy: 0.4163 - val_loss: 1.7098 - learning_rate: 1.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 54ms/step - accuracy: 0.3100 - loss: 1.9224 - val_accuracy: 0.4219 - val_loss: 1.7081 - learning_rate: 1.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 51ms/step - accuracy: 0.3102 - loss: 1.9187 - val_accuracy: 0.4199 - val_loss: 1.7087 - learning_rate: 1.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3122 - loss: 1.9190 - val_accuracy: 0.4203 - val_loss: 1.7029 - learning_rate: 5.0000e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3133 - loss: 1.9229 - val_accuracy: 0.4195 - val_loss: 1.7048 - learning_rate: 5.0000e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.3147 - loss: 1.9142 - val_accuracy: 0.4210 - val_loss: 1.7043 - learning_rate: 5.0000e-05\n",
            "Epoch 31/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 54ms/step - accuracy: 0.3124 - loss: 1.9096 - val_accuracy: 0.4248 - val_loss: 1.7012 - learning_rate: 5.0000e-05\n",
            "Epoch 32/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.3154 - loss: 1.9134 - val_accuracy: 0.4228 - val_loss: 1.6992 - learning_rate: 5.0000e-05\n",
            "Epoch 33/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 54ms/step - accuracy: 0.3162 - loss: 1.9119 - val_accuracy: 0.4251 - val_loss: 1.6985 - learning_rate: 5.0000e-05\n",
            "Epoch 34/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 55ms/step - accuracy: 0.3129 - loss: 1.9111 - val_accuracy: 0.4256 - val_loss: 1.6954 - learning_rate: 5.0000e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 54ms/step - accuracy: 0.3093 - loss: 1.9238 - val_accuracy: 0.4235 - val_loss: 1.6935 - learning_rate: 5.0000e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.3176 - loss: 1.9210 - val_accuracy: 0.4257 - val_loss: 1.6945 - learning_rate: 5.0000e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.3097 - loss: 1.9121 - val_accuracy: 0.4266 - val_loss: 1.6961 - learning_rate: 5.0000e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.3135 - loss: 1.9115 - val_accuracy: 0.4258 - val_loss: 1.6918 - learning_rate: 5.0000e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3182 - loss: 1.9094 - val_accuracy: 0.4252 - val_loss: 1.6906 - learning_rate: 5.0000e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - accuracy: 0.3143 - loss: 1.9132 - val_accuracy: 0.4288 - val_loss: 1.6926 - learning_rate: 5.0000e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.3109 - loss: 1.9107 - val_accuracy: 0.4304 - val_loss: 1.6915 - learning_rate: 5.0000e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 55ms/step - accuracy: 0.3158 - loss: 1.9049 - val_accuracy: 0.4295 - val_loss: 1.6908 - learning_rate: 5.0000e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.3143 - loss: 1.9124 - val_accuracy: 0.4274 - val_loss: 1.6905 - learning_rate: 2.5000e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3148 - loss: 1.9066 - val_accuracy: 0.4284 - val_loss: 1.6879 - learning_rate: 2.5000e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 52ms/step - accuracy: 0.3167 - loss: 1.9067 - val_accuracy: 0.4271 - val_loss: 1.6924 - learning_rate: 2.5000e-05\n",
            "Epoch 46/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 52ms/step - accuracy: 0.3178 - loss: 1.9001 - val_accuracy: 0.4290 - val_loss: 1.6879 - learning_rate: 2.5000e-05\n",
            "Epoch 47/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.3189 - loss: 1.9054 - val_accuracy: 0.4292 - val_loss: 1.6891 - learning_rate: 2.5000e-05\n",
            "Epoch 48/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 52ms/step - accuracy: 0.3213 - loss: 1.9029 - val_accuracy: 0.4292 - val_loss: 1.6883 - learning_rate: 1.2500e-05\n",
            "Epoch 49/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - accuracy: 0.3202 - loss: 1.9027 - val_accuracy: 0.4303 - val_loss: 1.6866 - learning_rate: 1.2500e-05\n",
            "Epoch 50/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - accuracy: 0.3172 - loss: 1.9055 - val_accuracy: 0.4290 - val_loss: 1.6869 - learning_rate: 1.2500e-05\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64), epochs=50,\n",
        "                    validation_data=(x_test, y_test), callbacks=[reduce_lr, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2D8Vq4F1eqT",
        "outputId": "ae320fe1-8543-4179-cabc-113070ef349a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save('model_name.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze some layers of the base model for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Re-compile the model with a lower learning rate for fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3N2mwelQ10hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwBLQprH2YGD",
        "outputId": "d01142bb-abc1-4341-cec6-3bc0d15c450a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 100ms/step - accuracy: 0.1412 - loss: 3.4917 - val_accuracy: 0.1238 - val_loss: 4.2903 - learning_rate: 1.0000e-05\n",
            "Epoch 2/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 66ms/step - accuracy: 0.2353 - loss: 2.5482 - val_accuracy: 0.3582 - val_loss: 1.7889 - learning_rate: 1.0000e-05\n",
            "Epoch 3/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 64ms/step - accuracy: 0.2854 - loss: 2.2141 - val_accuracy: 0.4148 - val_loss: 1.6271 - learning_rate: 1.0000e-05\n",
            "Epoch 4/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.3363 - loss: 1.9791 - val_accuracy: 0.4591 - val_loss: 1.4920 - learning_rate: 1.0000e-05\n",
            "Epoch 5/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.3775 - loss: 1.8060 - val_accuracy: 0.4991 - val_loss: 1.3800 - learning_rate: 1.0000e-05\n",
            "Epoch 6/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 63ms/step - accuracy: 0.4162 - loss: 1.6797 - val_accuracy: 0.5286 - val_loss: 1.2949 - learning_rate: 1.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 62ms/step - accuracy: 0.4514 - loss: 1.5607 - val_accuracy: 0.5592 - val_loss: 1.2142 - learning_rate: 1.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 66ms/step - accuracy: 0.4899 - loss: 1.4649 - val_accuracy: 0.5883 - val_loss: 1.1608 - learning_rate: 1.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 64ms/step - accuracy: 0.5141 - loss: 1.3916 - val_accuracy: 0.6118 - val_loss: 1.1077 - learning_rate: 1.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 63ms/step - accuracy: 0.5444 - loss: 1.3262 - val_accuracy: 0.6285 - val_loss: 1.0625 - learning_rate: 1.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 63ms/step - accuracy: 0.5654 - loss: 1.2610 - val_accuracy: 0.6466 - val_loss: 1.0196 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 66ms/step - accuracy: 0.5881 - loss: 1.2028 - val_accuracy: 0.6529 - val_loss: 0.9916 - learning_rate: 1.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m162/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 64ms/step - accuracy: 0.6043 - loss: 1.1422"
          ]
        }
      ],
      "source": [
        "# Fine-tune the model\n",
        "history_fine = model.fit(datagen.flow(x_train, y_train, batch_size=64), epochs=30,\n",
        "                         validation_data=(x_test, y_test),callbacks=[reduce_lr, early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate predictions and classification report\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_true, y_pred_classes))\n",
        "\n"
      ],
      "metadata": {
        "id": "9EJVXkaa22_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "uhPuBg703_kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8smBIZgb1vI",
        "outputId": "932173d3-a166-4ab4-b469-3849a98dd24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 16ms/step - accuracy: 0.1006 - loss: 3.6680\n",
            "Fine-tuned Model Test Accuracy: 0.1000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate the fine-tuned model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Fine-tuned Model Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "ux5Gz19SrDQD",
        "outputId": "5184557c-152c-415c-a408-2eb77b1370d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Function to load and preprocess the image\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(32, 32))  # Resize to 32x32 pixels\n",
        "    img_array = image.img_to_array(img)  # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = img_array.astype('float32') / 255.0  # Normalize pixel values\n",
        "    return img, img_array\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_image(img_path, model):\n",
        "    # Load and preprocess the image\n",
        "    img, img_array = load_and_preprocess_image(img_path)\n",
        "\n",
        "    # Make a prediction\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    # Get the index of the class with the highest probability\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Mapping of CIFAR-10 class indices to human-readable labels\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    # Get the class name\n",
        "    predicted_label = class_names[predicted_class[0]]\n",
        "\n",
        "    # Display the image with the prediction label\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Predicted: {predicted_label}\")\n",
        "    plt.axis('off')  # Hide the axes\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "img_path = 'bird.jpg'\n",
        "predict_image(img_path, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72JSlHjas6vI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def load_display_and_preprocess_image(img_path):\n",
        "\n",
        "    original_img = image.load_img(img_path)\n",
        "\n",
        "    # Resize the image to 32x32 pixels for prediction\n",
        "    img = image.load_img(img_path, target_size=(32, 32))\n",
        "    img_array = image.img_to_array(img)  # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = img_array.astype('float32') / 255.0  # Normalize pixel values\n",
        "\n",
        "    return original_img, img_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess the image\n",
        "def load_and_preprocess_image(img_path):\n",
        "    original_img = image.load_img(img_path)\n",
        "\n",
        "    img = image.load_img(img_path, target_size=(32, 32))  # Resize to 32x32 pixels\n",
        "    img_array = image.img_to_array(img)  # Convert to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = img_array.astype('float32') / 255.0  # Normalize pixel values\n",
        "    return img, img_array"
      ],
      "metadata": {
        "id": "dOEgUrfm7E5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_display(img_path, model):\n",
        "    original_img, img_array = load_display_and_preprocess_image(img_path)\n",
        "    # Make a prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions, axis=1)\n",
        "\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    # Get the class name\n",
        "    predicted_label = class_names[predicted_class[0]]\n",
        "    # Display the original image with the prediction label\n",
        "    plt.imshow(original_img)\n",
        "    plt.title(f\"Predicted: {predicted_label}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9ppCfLqH7vMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = 'bird.jpg'\n",
        "predict_image(img_path, model)"
      ],
      "metadata": {
        "id": "U8VDXvMB8WCz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}